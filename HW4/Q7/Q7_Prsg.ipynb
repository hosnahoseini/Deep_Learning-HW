{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>HW4 - Attention and Transformers- Practical Q2</h1>\n",
    "<h3><font color=yellow>Total Points: 100</font></h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyRAPQ2jz3uu"
   },
   "source": [
    "<font size=+4>\n",
    "Introduction\n",
    "</font>\n",
    "<br/>\n",
    "<font size=+2>\n",
    "\n",
    "<br/>\n",
    "Part-of-speech tagging (POS tagging) is a natural language processing task where each word in a given text is assigned a specific part-of-speech category, such as noun, verb, adjective, etc. In the context of Roberta, a state-of-the-art language model based on the transformer architecture, POS tagging involves leveraging the model's pre-trained knowledge to accurately predict and label the grammatical roles of words in a sentence. Roberta's extensive training data and attention mechanisms enable it to capture subtle contextual cues, allowing for more nuanced and accurate POS tagging. By understanding the syntactic structure of a sentence, Roberta enhances the efficiency and precision of POS tagging, contributing to more sophisticated language understanding and downstream applications in natural language processing.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gadCMzj1sJNx"
   },
   "source": [
    "<font size=+2>\n",
    "In this section we want to fine-tune the Roberta that you trained in previous section for POS-Tagging Task on Parsig dataset\n",
    "<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mq-uxun1OT8z",
    "outputId": "c55ca8b5-695a-499d-f550-461d933b6729"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate -U\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8vC1vyA1NHV"
   },
   "outputs": [],
   "source": [
    "# All label that exist\n",
    "# AAX mean Unknown used for CLS and other symbols\n",
    "tags = {'AAX', 'B-ADJ', 'I-ADJ', 'B-ADV', 'I-ADV', 'B-N', 'I-N', 'B-V', 'I-V',\\\n",
    "        'B-PRONOUN', 'I-PRONOUN', 'B-NUM', 'I-NUM', 'B-DET', 'I-DET', 'B-PRE',\\\n",
    "        'I-PRE', 'B-POST', 'I-POST', 'B-CONJ', 'I-CONJ', 'B-JUNK', 'I-JUNK',\\\n",
    "        'B-MARKER', 'I-MARKER', }\n",
    "# Convert lables to speceif numeric id\n",
    "tag2id = {tag: id for id, tag in enumerate(sorted(list(tags)))}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBvz2ThQOpjW",
    "outputId": "ffce3b80-7e16-444e-8364-b83b962d31fc"
   },
   "outputs": [],
   "source": [
    "######################   TODO 1.1   ########################\n",
    "# Load pretrained model that trained in previous sectiopn\n",
    "# Load tokenizer from previous section\n",
    "# Freeze base model for fine-tuning\n",
    "###################### (5 points) ##########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riyBVLBZ23mB"
   },
   "outputs": [],
   "source": [
    "######################   TODO 1.2  ########################\n",
    "# Load datasets\n",
    "###################### (5 points) ##########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3kY0xzOaOGE"
   },
   "outputs": [],
   "source": [
    "######################   TODO 1.3   ########################\n",
    "# Complete custom data set\n",
    "# Use Torch dataloader for datasets\n",
    "###################### (15 points) ##########################\n",
    "class CustomDataset:\n",
    "    def __init__(self, df):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27zeBD6shL9t"
   },
   "outputs": [],
   "source": [
    "######################   TODO 2.1   ########################\n",
    "# Write a code for training model with training datset\n",
    "# At each epoch report accurcy for validation dataset\n",
    "# Save best model by accuracy\n",
    "###################### (30 points) ##########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4hXNxvjpx9I"
   },
   "outputs": [],
   "source": [
    "######################   TODO 2.2   ########################\n",
    "# Report best model accuarcy on validation dataset\n",
    "# Accuracy below 85% will not be graded\n",
    "###################### (5 points) ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWT9DkkOqKCn"
   },
   "outputs": [],
   "source": [
    "######################   TODO 2.1   ########################\n",
    "# Now implement it with huggingface trainer\n",
    "###################### (40 points) ##########################"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
